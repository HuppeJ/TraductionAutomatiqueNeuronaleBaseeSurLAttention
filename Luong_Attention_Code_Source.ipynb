{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Luong-Attention-Code-Source.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Effective Approaches to Attention-based Neural Machine Tranlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EKPF7f1146yA"
      },
      "source": [
        "L'objectif de ce travail consiste à explorer les résultats obtenus dans l'article [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) et de reproduire certain des ces résultats. \n",
        "\n",
        "Certaines parties de ce notebook sont basées sur le tutoriel [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention).\n",
        "\n",
        "\n",
        "Ce notebook détail les implémentations des modèles utilisés pour la traduction de phrase d'anglais à français. Le dataset utilisé pour l'entraînement et l'évaluation de nos modèles provient du site http://www.manythings.org/anki/.\n",
        "\n",
        "\n",
        "Travail présenté par : \n",
        "\n",
        "- Fabrice Charbonneau\n",
        "- Antoine Daigneault-Demers\n",
        "- Jérémie Huppé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4mEZlYvi1ONZ"
      },
      "source": [
        "## Détails d'implémentation\n",
        "\n",
        "### Encodeur \n",
        "- Un *stack* de 2 couches LSTM de 1000 *units* avec dropout de 0.2 optionnel\n",
        "\n",
        "### Attention \n",
        "\n",
        "Deux implémentations de méchanismes d'attention sont présentées dans ce notebook :\n",
        "\n",
        "- Attention *globale* avec score *dot*\n",
        "- Attention *local-p* avec score *general* et une taille de fenêtre D = 10\n",
        "\n",
        "### Décodeur \n",
        "- Un *stack* de 2 couches LSTM de 1000 *units* avec dropout de 0.2 optionnel\n",
        "- Avec ou sans un méchanisme d'attention pour la prédiction\n",
        "- Utilisation du principe de *teacher forcing* lors de l'entraînement\n",
        "\n",
        "### Paramètres d'entraînement\n",
        "\n",
        "Voici les paramètres utilisés lors de l'entraînement des différents modèles :\n",
        "\n",
        "- BATCH_SIZE = 128\n",
        "- embedding_dim = 1000\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWHHaPmz_grS",
        "colab": {}
      },
      "source": [
        "REVERSE = False\n",
        "DROPOUT = True\n",
        "ATTENTION = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eklpI_Ck5QtH"
      },
      "source": [
        "## Implémentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Lz3txWz47z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Le dataset French - English fra-eng.zip (175623) de http://www.manythings.org/anki/ a été utilisé pour entrainer nos modèles.\n",
        "\n",
        "Le preprocessing utilisé dans le tutoriel [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention) a été réutilisé pour ce travail.\n",
        "\n",
        "Ce preprocessing consite à :\n",
        "\n",
        "1. Ajouter un token *start* et un token *end* à chaque phrase.\n",
        "2. Enlever les caractères spéciaux des phrases.\n",
        "3. Créer un index des mots et un index inversé afin d'obtenir un dictionnaire mot → id et un autre dictionnaire id → mot.\n",
        "4. Ajouter du remplissage pour que chaque phrase ait comme taille la taille de la plus grande phrase du dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Ky_lvO1yo26",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ve4E0Pby28N",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "root_folder = \"drive/My Drive/INF8225_Project_Shared_Folder/\"\n",
        "path_to_zip = root_folder + \"fra-eng.zip\"\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(root_folder)\n",
        "\n",
        "path_to_file = os.path.dirname(root_folder)+\"/fra.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDOSaDI1C6bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ouput_dataframe(df, csv_name):\n",
        "    output_file_path = [PROJECT_PATH, csv_name]\n",
        "    output_file = os.path.join('', *output_file_path)\n",
        "    df.to_csv(output_file, sep=',', encoding='utf-8', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "# Preprocessing provenant du tutoriel Neural machine translation with attention \n",
        "# Lien : https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w, reverse=False):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # Reverse the sentence's words\n",
        "    if reverse:\n",
        "        w = \" \".join(w.split()[::-1])\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, FRENCH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = []\n",
        "    for l in lines[:num_examples]:\n",
        "        sentences = l.split('\\t')\n",
        "        #                                       Source (input)  (ENGLISH)               Target (output) (FRENCH) \n",
        "        word_pairs.append([preprocess_sentence(sentences[1], reverse=False), preprocess_sentence(sentences[0], reverse=False)])\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmMZQpdO60dt",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "    # Create language tokenizers and extract tensors\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Définir la taille du dataset lors de sa création \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# La taille du dataset a été fixée à num_examples :\n",
        "num_examples = 60000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "colab": {}
      },
      "source": [
        "# Création du training and validation sets en utilisant un split 80-20\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hSvoAHDNIm_y",
        "colab": {}
      },
      "source": [
        "def tensor_to_sentence(lang, tensor_value):\n",
        "    output = []\n",
        "    for index, value in enumerate(tensor_value): \n",
        "        if value != 0:\n",
        "            output.append(lang.index_word[value])\n",
        "    \n",
        "    return output[1:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "colab": {}
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLJAh9XGJLtQ",
        "colab": {}
      },
      "source": [
        "# Les paramètres ci-dessous sont les paramètres utilisés dans l'article. \n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 1000\n",
        "units = 1000\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U00m-VzpC6cQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHB3Xofj9pmZ"
      },
      "source": [
        "## Implémentation de l'architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoCgYYpy927h"
      },
      "source": [
        "### Encodeur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.dropout = dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        if DROPOUT:\n",
        "            dropout_val = 0.2\n",
        "        else: \n",
        "            dropout_val = 0\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "        self.lstm2 = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "\n",
        "    def call(self, x, encoder_states):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        e_outputs, h1, c1 = self.lstm1(x, initial_state=encoder_states[0])\n",
        "\n",
        "        whole_sequence_output, h2, c2 = self.lstm2(e_outputs, initial_state=encoder_states[1])\n",
        "\n",
        "        encoder_states = [[h1, c1], [h2, c2]]\n",
        "\n",
        "        return whole_sequence_output, encoder_states\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return [[tf.zeros((self.batch_sz, self.enc_units)) for i in range(2)] for j in range(2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "colab": {}
      },
      "source": [
        "# Démo Encoder \n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, DROPOUT)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden_states = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder state shape: h2 : (batch size, units) {}'.format(sample_hidden_states[1][0].shape))\n",
        "print ('Encoder state shape: c2 : (batch size, units) {}'.format(sample_hidden_states[1][1].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZaYwwKWqCSyy"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVgJf9Dbz2bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LuongLocalpAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongLocalpAttention, self).__init__()\n",
        "        self.W_p = tf.keras.layers.Dense(units)\n",
        "        self.v_p = tf.keras.layers.Dense(1)\n",
        "        self.W_a = tf.keras.layers.Dense(units)\n",
        "\n",
        "\n",
        "    def non_zero_softmax_on_row(self, row_tensor): \n",
        "        # Inputs : \n",
        "        #   row_tensor shape : (max_length_sentence, 1)\n",
        "\n",
        "        # Création d'une partition basée sur la condition non_zeros :\n",
        "        non_zeros_bool = ~tf.equal(row_tensor, 0.)\n",
        "        non_zeros_int = tf.cast(non_zeros_bool, tf.int32)\n",
        "        partitioned_tensor = tf.dynamic_partition(row_tensor, non_zeros_int, 2)\n",
        "\n",
        "        # Application du softmax seulement sur les valeurs non égales à 0.\n",
        "        partitioned_tensor[1] = tf.nn.softmax(partitioned_tensor[1])\n",
        "\n",
        "        # Trouver les indices à remplacer les résultats du softmax\n",
        "        condition_indices = tf.dynamic_partition(tf.range(tf.size(row_tensor)), tf.reshape(non_zeros_int, [-1]), 2)\n",
        "\n",
        "        # Effectuer le remplacement\n",
        "        softmax_row_tensor = tf.dynamic_stitch(condition_indices, partitioned_tensor)\n",
        "        softmax_row_tensor = tf.math.abs(softmax_row_tensor) # remove -0.0 values\n",
        "        softmax_row_tensor = tf.reshape(softmax_row_tensor, tf.shape(row_tensor))\n",
        "\n",
        "        return softmax_row_tensor\n",
        "\n",
        "    def non_zero_softmax_on_matrix(self, matrix_tensor):\n",
        "        return tf.map_fn(self.non_zero_softmax_on_row, matrix_tensor)\n",
        "\n",
        "    def call(self, dec_hidden_last_layer, enc_output):\n",
        "        # Inputs : \n",
        "        #   dec_hidden = ht du dernier layer  -> (batch_size, hidden_size)\n",
        "        #   enc_output = all hs               -> (batch_size, max_len, hidden_size) \n",
        "        #   S est la longueur des phrases (longueur de la plus longue phrase)\n",
        "        #   D est la taille de la fenetre (window)\n",
        "\n",
        "        batch_size = enc_output.shape[0]\n",
        "        S = enc_output.shape[1] \n",
        "        D = 10.0\n",
        "        sigma = D / 2.0\n",
        "\n",
        "        dec_hidden_last_layer_time_axis = tf.expand_dims(dec_hidden_last_layer, axis=1) # (batch_size, 1, hidden_size)   \n",
        "\n",
        "        # Trouvons pt. pt = S * sig(vp * tanh(Wp ht))\n",
        "        pt = self.W_p(dec_hidden_last_layer_time_axis)  \n",
        "        pt = tf.keras.activations.tanh(pt)                      # (batch_size, 1, hidden_size)\n",
        "        pt = self.v_p(pt)                                       # (batch_size, 1, 1)\n",
        "        pt = tf.keras.activations.sigmoid(pt)                   # (batch_size, 1, 1)\n",
        "        pt = pt * (S-1)                                         # (batch_size, 1, 1)   \n",
        "\n",
        "\n",
        "        pt_copy = tf.identity(pt)                               # (batch_size, 1, 1)\n",
        "        multiplies = tf.constant([1, S, 1], tf.int32)  \n",
        "        pt_matrix = tf.tile(pt_copy, multiplies)                # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir une matrice de contenant les indices de chaque colonne dans chaque ligne\n",
        "        row_indexes = [np.arange(S)]\n",
        "        matrix_row_indexes = np.repeat(row_indexes, batch_size, axis=0)                 # (batch_size, max_sent_length, 1)\n",
        "        matrix_row_indexes = np.expand_dims(matrix_row_indexes, axis=2)\n",
        "        matrix_row_indexes = tf.convert_to_tensor(matrix_row_indexes, dtype=tf.float32)\n",
        "\n",
        "        # Trouvons la matrice window_one_hot_matrix avec un 1 aux indices des mots qui doivent\n",
        "        # être considérés pour le calcul de l'attention\n",
        "        window_pt_diff = tf.math.subtract(matrix_row_indexes, pt)\n",
        "        window_pt_abs = tf.math.abs(window_pt_diff)\n",
        "        window_one_hot_matrix = tf.where(tf.less(window_pt_abs, D), 1.0, 0.0)           # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir poids de la gaussienne\n",
        "        s_minus_pt = tf.math.subtract(matrix_row_indexes, pt_matrix)\n",
        "        s_minus_pt_pow_2 = tf.map_fn(lambda x: x*x, s_minus_pt)\n",
        "        num_divided_by_denum = tf.math.divide(s_minus_pt_pow_2, 2*(sigma**2))\n",
        "        gaussian = tf.math.exp(-num_divided_by_denum)                                   # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir poids de la gaussienne seulement pour les mots faisant partie \n",
        "        # du calcul de l'attention\n",
        "        gaussian_window = tf.math.multiply(gaussian, window_one_hot_matrix)             # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Calcul du score\n",
        "        score = self.W_a(enc_output)    # score (Wa@hs) score.shape = (batch_size, max_sent_length, hidden_size)\n",
        "        score = tf.keras.layers.Dot(axes=[2, 2])([score, dec_hidden_last_layer_time_axis])  # score ht.T Wa@hs score.shape = (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir seulement les score faisant partie des mots à considérer\n",
        "        # pour le calcul de l'attention\n",
        "        score_one_hot = tf.math.multiply(window_one_hot_matrix, score)\n",
        "\n",
        "        # Appliquer softmax seulement sur les éléments de chaque ligne non égal à 0.\n",
        "        # Ces éléments représentent le score de chaque mot qui doit être considéré \n",
        "        # pour le calcul de l'attention\n",
        "        align_ht_hs = self.non_zero_softmax_on_matrix(score_one_hot)\n",
        "\n",
        "        # Obtenir les poids de l'attention en multipliant élément par élément \n",
        "        # les poids de la gaussienne au éléments de align_ht_hs\n",
        "        attention_weights = tf.math.multiply(gaussian_window, align_ht_hs) # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Trouver ct = (batch_size, hidden_size) \n",
        "        # Détails : hs = (batch_size, max_sent_length, hidden_size) * at = (batch_size, max_sent_length, 1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1) # (BATCH_SIZE, 1000)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class LuongGlobalAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongGlobalAttention, self).__init__()\n",
        "        self.W_p = tf.keras.layers.Dense(units)\n",
        "        self.v_p = tf.keras.layers.Dense(1)\n",
        "        self.W_a = tf.keras.layers.Dense(units)\n",
        "\n",
        "\n",
        "    def call(self, dec_hidden_last_layer, enc_output):\n",
        "        # Inputs : \n",
        "        #   dec_hidden = ht du dernier layer  -> (batch_size, hidden_size)\n",
        "        #   enc_output = all hs               -> (batch_size, max_len, hidden_size) \n",
        "\n",
        "        dec_hidden_last_layer_time_axis = tf.expand_dims(dec_hidden_last_layer, axis=1) # (batch_size, 1, hidden_size)   \n",
        "\n",
        "        # Calcul du score (dot)\n",
        "        score = tf.keras.layers.Dot(axes=[2, 2])([enc_output, dec_hidden_last_layer_time_axis])  # score ht.T Wa@hs score.shape = (batch_size, max_sent_length, 1)\n",
        "\n",
        "        align_ht_hs = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Obtenir les poids de l'attention\n",
        "        attention_weights = align_ht_hs  # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Trouver ct = (batch_size, hidden_size) \n",
        "        # Détails : hs = (batch_size, max_sent_length, hidden_size) * at = (batch_size, max_sent_length, 1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1) # (BATCH_SIZE, hidden_size)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "colab": {}
      },
      "source": [
        "# Démo Attention Layer \n",
        "\n",
        "attention_layer = LuongGlobalAttention(units)\n",
        "\n",
        "def get_dec_hidden_last_layer_time_axis():\n",
        "    return tf.random.uniform((BATCH_SIZE, units), minval=0, maxval=1, dtype=tf.float32, seed=64)\n",
        "\n",
        "dec_hidden_last_layer_time_axis = get_dec_hidden_last_layer_time_axis()\n",
        "attention_result, attention_weights = attention_layer(dec_hidden_last_layer_time_axis, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPKF5EWnCYV4"
      },
      "source": [
        "### Décodeur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.dropout = dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.W_c = tf.keras.layers.Dense(self.dec_units)\n",
        "\n",
        "        if dropout:\n",
        "            dropout_val = 0.2\n",
        "        else: \n",
        "            dropout_val = 0\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "        self.lstm2 = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = LuongGlobalAttention(self.dec_units)\n",
        "\n",
        "\n",
        "    def call(self, dec_input, dec_hidden, enc_output):\n",
        "        # Inputs:\n",
        "        #   dec_input shape == (batch_size, max_length, dec_hidden_size)\n",
        "        #   dec_hidden shape == (batch_size, dec_hidden_size)\n",
        "        #   enc_output shape == (batch_size, max_length, dec_hidden_size)\n",
        "\n",
        "        # x après le layer de embeddings == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(dec_input)\n",
        "\n",
        "        d_outputs, dh1, dc1 =  self.lstm1(x, initial_state=dec_hidden[0])\n",
        "        d_outputs_final, dh2, dc2 = self.lstm2(d_outputs, initial_state=dec_hidden[1]) \n",
        "\n",
        "        decoder_states = [[dh1, dc1], [dh2, dc2]]\n",
        "\n",
        "        if ATTENTION:\n",
        "            # Évaluer ht_hat = tanh(Wc[concat(ct,ht)])\n",
        "\n",
        "            # output shape == (batch_size * 1, hidden_size)\n",
        "            output = tf.reshape(d_outputs_final, (-1, d_outputs_final.shape[2]))\n",
        "\n",
        "            context_vector, attention_weights = self.attention(output, enc_output) \n",
        "            ct_ht = tf.concat([context_vector, output], axis=-1) # ct_ht shape == (batch_size, embedding_dim + dec_hidden_size)\n",
        "            Wc_ct_ht = self.W_c(ct_ht)\n",
        "            ht_hat =  tf.keras.activations.tanh(Wc_ct_ht)   # ht_hat shape == (batch_size, embedding_dim)\n",
        "\n",
        "            # output shape == (batch_size, embedding_dim)\n",
        "            output = ht_hat\n",
        "        else: \n",
        "            attention_weights = None\n",
        "            # output shape == (batch_size * 1, embedding_dim)\n",
        "            output = tf.reshape(d_outputs_final, (-1, d_outputs_final.shape[2]))\n",
        "\n",
        "\n",
        "        Ws_ht = self.fc(output) # Ws_ht shape == (batch_size, vocab)\n",
        "\n",
        "        return Ws_ht, decoder_states, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "colab": {}
      },
      "source": [
        "# Démo Decoder \n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, DROPOUT)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden_states, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Fonction de perte et optimiseur "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Entraînement\n",
        "\n",
        "1. Passez *l'entrée* à travers *l'encodeur* qui retourne *la sortie de l'encodeur* et *l'état caché de l'encodeur*.\n",
        "2. La sortie de l'encoder, l'état caché de l'encodeur et l'entrée du décodeur (qui est le token *start*) sont transmis au décodeur.\n",
        "3. 3. Le décodeur renvoie les prédictions et l'état caché du décodeur.\n",
        "4. L'état caché du décodeur est ensuite passée une fois de plus dans le modèle et les prédicitions sont utilisées pour évaluer la loss.\n",
        "5. Le principe de *teacher forcing* est utilisé pour déterminer le prochain entré du décodeur.\n",
        "6. La dernière étape consiste à évaluer le gradient et l'appliquer à l'optimisateur pour ensuite procéder à la backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "# La boucle d'entraînment provient du tutoriel Neural machine translation with attention \n",
        "# et a été adapté pour notre architecture.\n",
        "# Lien : https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, target, enc_states):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_states = encoder(inp, enc_states)\n",
        "\n",
        "        dec_hidden = enc_states\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, target.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "col_names =  ['Epoch', 'Loss']\n",
        "loss_df  = pd.DataFrame(columns = col_names)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_states = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_states)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0: \n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        " \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    loss_df.loc[len(loss_df)] = [epoch + 1, K.eval(total_loss / steps_per_epoch)] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AI_IaVxCHF5z",
        "colab": {}
      },
      "source": [
        "if ATTENTION:\n",
        "    csv_name = \"tableau_loss_with_attention.csv\"\n",
        "    loss_df.to_csv(csv_name)\n",
        "    !cp tableau_loss_with_attention.csv \"drive/My Drive/\"\n",
        "else:  \n",
        "    csv_name = \"tableau_loss_without_attention.csv\"   \n",
        "    loss_df.to_csv(csv_name)\n",
        "    !cp tableau_loss_without_attention.csv \"drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Traduction\n",
        "\n",
        "* La fonction d'évaluation est similaire à la boucle d'entraînement, sauf que nous n'utilisons pas le *teacher forcing*. L'entrée du décodeur à chaque pas de temps t correspond à ses prédictions précédentes ainsi qu'à l'état caché et à la sortie de l'encodeur.\n",
        "* La prédiction pour la traduction arrête lorsque le *end token* est atteint.\n",
        "* Les *attention weights* sont enregistrés pour chaque instant t.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "# Les fonctions utilisées pour la traduction proviennent du tutoriel Neural machine translation with attention \n",
        "# et ont été adaptées pour notre architecture et nos traitements.\n",
        "# Lien : https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence, REVERSE)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [[tf.zeros((1, units)) for i in range(2)] for j in range(2)]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        if attention_weights is None:\n",
        "            attention_plot = None\n",
        "        else:\n",
        "            attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "            attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence, print_attention=True):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    if attention_plot is not None and print_attention:\n",
        "        attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "        plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restoration du dernier checkpoint pour tester"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {}
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZM0Vj_jRGF7",
        "colab": {}
      },
      "source": [
        "translate(u'I recovered')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0BDjy2ew0Kkm",
        "colab": {}
      },
      "source": [
        "translate(u'You re the oldest.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izleX7rf0KiT",
        "colab": {}
      },
      "source": [
        "translate(u'He walks to school .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2wJe8Oh0Kg1",
        "colab": {}
      },
      "source": [
        "translate(u'Make a list .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rkGG1vXL0Kdp",
        "colab": {}
      },
      "source": [
        "translate(u'Is it your car .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YBsQmkhT0KbW",
        "colab": {}
      },
      "source": [
        "translate(u'He lied to me .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOu-FwxL0KZH",
        "colab": {}
      },
      "source": [
        "translate(u'I didn t laugh .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p22f-uOi0KWd",
        "colab": {}
      },
      "source": [
        "translate(u'I work with him .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DTJcyc-c0KTz",
        "colab": {}
      },
      "source": [
        "translate(u'I just moved here .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2uClBJEu0KLZ",
        "colab": {}
      },
      "source": [
        "translate(u'I felt left out .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zI4ENwFz1vpH",
        "colab": {}
      },
      "source": [
        "translate(u'What did you make .')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j2D32bYB4mWZ"
      },
      "source": [
        "## BLEU score sur test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JADVb-8L4qX4",
        "colab": {}
      },
      "source": [
        "# input_tensor_val: phrase en entrée\n",
        "# target_tensor_val : phrase attendue\n",
        "\n",
        "smoother = SmoothingFunction()\n",
        "\n",
        "BLEU_scores = {}\n",
        "\n",
        "col_names =  ['Length', 'BLEUscore']\n",
        "BLEU_df  = pd.DataFrame(columns = col_names)\n",
        "\n",
        "\n",
        "col_names =  ['id', 'src', 'result', 'reference_str', 'sentence_length', 'BLEUscore']\n",
        "sentence_sample_df  = pd.DataFrame(columns=col_names)\n",
        "\n",
        "nb_examples = len(input_tensor_val)\n",
        "\n",
        "for i in range(nb_examples):\n",
        "    # On fait la traduction\n",
        "    converted_sentence = tensor_to_sentence(inp_lang, input_tensor_val[i])\n",
        "    sentence_length = str(len(converted_sentence))\n",
        "\n",
        "    src = \" \".join(converted_sentence)\n",
        "    result = translate(src, print_attention=False).strip()\n",
        "    hypothesis = result.split(\" \")[:-1]\n",
        "    reference = tensor_to_sentence(targ_lang, target_tensor_val[i])\n",
        "    reference_str = \" \".join(reference)\n",
        "\n",
        "\n",
        "    # Il peut y avoir plusieurs références, mais pas dans notre cas\n",
        "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, smoothing_function=smoother.method1)\n",
        "\n",
        "    # Enregister certaines traductions pour une analyse manuelle\n",
        "    percentage = 0.01\n",
        "    if i % (int(percentage * nb_examples)) == 0:\n",
        "        idx = i // (int(percentage * nb_examples))\n",
        "        sentence_sample_df.loc[idx] = [idx, src, result, reference_str, sentence_length, BLEUscore] \n",
        "\n",
        "    if str(sentence_length) in BLEU_scores:\n",
        "        occurences = BLEU_scores[sentence_length][0] + 1\n",
        "        total = BLEU_scores[sentence_length][1] + BLEUscore\n",
        "        BLEU_scores[sentence_length] = (occurences, total)\n",
        "    else:\n",
        "        BLEU_scores[sentence_length] = (1, BLEUscore)\n",
        "\n",
        "lengths = []\n",
        "BLEU_averages = []\n",
        "\n",
        "for sentence_length, (occurences, total_BLEU_score) in BLEU_scores.items():\n",
        "    length = int(sentence_length)\n",
        "    average_BLEU_score = total_BLEU_score / occurences\n",
        "    lengths.append(length)\n",
        "    BLEU_averages.append(average_BLEU_score)\n",
        "    BLEU_df.loc[len(BLEU_df)] = [length, average_BLEU_score] \n",
        "\n",
        "lists = sorted(zip(*[lengths, BLEU_averages]))\n",
        "lengths, BLEU_averages = list(zip(*lists))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qTu4s5-THR-I",
        "colab": {}
      },
      "source": [
        "if ATTENTION:\n",
        "    csv_name = \"tableau_BLEU_with_attention.csv\"\n",
        "    BLEU_df.to_csv(csv_name)\n",
        "    !cp tableau_BLEU_with_attention.csv \"drive/My Drive/\"\n",
        "\n",
        "    csv_name = \"tableau_traductions_with_attention.csv\"\n",
        "    sentence_sample_df.to_csv(csv_name)\n",
        "    !cp tableau_traductions_with_attention.csv \"drive/My Drive/\"\n",
        "\n",
        "else:\n",
        "    csv_name = \"tableau_BLEU_without_attention.csv\"\n",
        "    BLEU_df.to_csv(csv_name)\n",
        "    !cp tableau_BLEU_without_attention.csv \"drive/My Drive/\"\n",
        "\n",
        "    csv_name = \"tableau_traductions_without_attention.csv\"\n",
        "    sentence_sample_df.to_csv(csv_name)\n",
        "    !cp tableau_traductions_without_attention.csv \"drive/My Drive/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lQqxFrXD3Glx",
        "colab": {}
      },
      "source": [
        "plt.plot(lengths, BLEU_averages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jUWMn-Nj0dFJ",
        "colab": {}
      },
      "source": [
        "global_BLEU_score = 0\n",
        "global_occurences = 0\n",
        "\n",
        "for sentence_length, (occurences, total_BLEU_score) in BLEU_scores.items():\n",
        "    global_BLEU_score += total_BLEU_score\n",
        "    global_occurences += occurences\n",
        "\n",
        "print(\"Average BLEU score:\", global_BLEU_score / global_occurences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VO725pbYacr8",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}