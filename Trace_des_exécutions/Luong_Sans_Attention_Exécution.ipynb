{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Luong-Sans-Attention-Exécution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Effective Approaches to Attention-based Neural Machine Tranlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EKPF7f1146yA"
      },
      "source": [
        "L'objectif de ce travail consiste à explorer les résultats obtenus dans l'article [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) et de reproduire certain des ces résultats. \n",
        "\n",
        "Certaines parties de ce notebook sont basées sur le tutoriel [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention).\n",
        "\n",
        "\n",
        "Ce notebook détail les implémentations des modèles utilisés pour la traduction de phrase d'anglais à français. Le dataset utilisé pour l'entraînement et l'évaluation de nos modèles provient du site http://www.manythings.org/anki/.\n",
        "\n",
        "\n",
        "Travail présenté par : \n",
        "\n",
        "- Fabrice Charbonneau\n",
        "- Antoine Daigneault-Demers\n",
        "- Jérémie Huppé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4mEZlYvi1ONZ"
      },
      "source": [
        "## Détails d'implémentation\n",
        "\n",
        "### Encodeur \n",
        "- Un *stack* de 2 couches LSTM de 1000 *units* avec dropout de 0.2 optionnel\n",
        "\n",
        "### Attention \n",
        "\n",
        "Deux implémentations de méchanismes d'attention sont présentées dans ce notebook :\n",
        "\n",
        "- Attention *globale* avec score *dot*\n",
        "- Attention *local-p* avec score *general* et une taille de fenêtre D = 10\n",
        "\n",
        "### Décodeur \n",
        "- Un *stack* de 2 couches LSTM de 1000 *units* avec dropout de 0.2 optionnel\n",
        "- Avec ou sans un méchanisme d'attention pour la prédiction\n",
        "- Utilisation du principe de *teacher forcing* lors de l'entraînement\n",
        "\n",
        "### Paramètres d'entraînement\n",
        "\n",
        "Voici les paramètres utilisés lors de l'entraînement des différents modèles :\n",
        "\n",
        "- BATCH_SIZE = 128\n",
        "- embedding_dim = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWHHaPmz_grS",
        "colab": {}
      },
      "source": [
        "REVERSE = False\n",
        "DROPOUT = True\n",
        "ATTENTION = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eklpI_Ck5QtH"
      },
      "source": [
        "## Implémentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "outputId": "79b97d86-d4e3-4471-a339-1c05d4c28344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Lz3txWz47z",
        "colab_type": "code",
        "outputId": "70e1387e-ddb8-4692-f6a2-4611c08855cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Le dataset French - English fra-eng.zip (175623) de http://www.manythings.org/anki/ a été utilisé pour entrainer nos modèles.\n",
        "\n",
        "Le preprocessing utilisé dans le tutoriel [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention) a été réutilisé pour ce travail.\n",
        "\n",
        "Ce preprocessing consite à :\n",
        "\n",
        "1. Ajouter un token *start* et un token *end* à chaque phrase.\n",
        "2. Enlever les caractères spéciaux des phrases.\n",
        "3. Créer un index des mots et un index inversé afin d'obtenir un dictionnaire mot → id et un autre dictionnaire id → mot.\n",
        "4. Ajouter du remplissage pour que chaque phrase ait comme taille la taille de la plus grande phrase du dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Ky_lvO1yo26",
        "outputId": "4f206f75-2028-4f72-a732-d5e9d2420f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ve4E0Pby28N",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "root_folder = \"drive/My Drive/INF8225_Project_Shared_Folder/\"\n",
        "path_to_zip = root_folder + \"fra-eng.zip\"\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(root_folder)\n",
        "\n",
        "path_to_file = os.path.dirname(root_folder)+\"/fra.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDOSaDI1C6bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ouput_dataframe(df, csv_name):\n",
        "    output_file_path = [PROJECT_PATH, csv_name]\n",
        "    output_file = os.path.join('', *output_file_path)\n",
        "    df.to_csv(output_file, sep=',', encoding='utf-8', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "# Preprocessing provenant du tutoriel Neural machine translation with attention \n",
        "# Lien : https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w, reverse=False):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # Reverse the sentence's words\n",
        "    if reverse:\n",
        "        w = \" \".join(w.split()[::-1])\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, FRENCH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = []\n",
        "    for l in lines[:num_examples]:\n",
        "        sentences = l.split('\\t')\n",
        "        #                                       Source (input)  (ENGLISH)               Target (output) (FRENCH) \n",
        "        word_pairs.append([preprocess_sentence(sentences[1], reverse=False), preprocess_sentence(sentences[0], reverse=False)])\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmMZQpdO60dt",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "    # Create language tokenizers and extract tensors\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Définir la taille du dataset lors de sa création \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# La taille du dataset a été fixée à num_examples :\n",
        "num_examples = 60000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "outputId": "33c174e0-80fc-42e5-ca1a-e5764ba112e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Création du training and validation sets en utilisant un split 80-20\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48000 48000 12000 12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hSvoAHDNIm_y",
        "colab": {}
      },
      "source": [
        "def tensor_to_sentence(lang, tensor_value):\n",
        "    output = []\n",
        "    for index, value in enumerate(tensor_value): \n",
        "        if value != 0:\n",
        "            output.append(lang.index_word[value])\n",
        "    \n",
        "    return output[1:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "outputId": "bfa29bb5-a362-400e-ccaa-38573f742f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "41 ----> ll\n",
            "27 ----> have\n",
            "13 ----> to\n",
            "98 ----> work\n",
            "176 ----> hard\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> je\n",
            "121 ----> vais\n",
            "1081 ----> devoir\n",
            "306 ----> travailler\n",
            "387 ----> dur\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLJAh9XGJLtQ",
        "colab": {}
      },
      "source": [
        "# Les paramètres ci-dessous sont les paramètres utilisés dans l'article. \n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 1000\n",
        "units = 1000\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U00m-VzpC6cQ",
        "colab_type": "code",
        "outputId": "b2673e69-041f-4de9-d18e-bdf9dc58cad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 12]), TensorShape([128, 19]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHB3Xofj9pmZ"
      },
      "source": [
        "## Implémentation de l'architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoCgYYpy927h"
      },
      "source": [
        "### Encodeur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.dropout = dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        if DROPOUT:\n",
        "            dropout_val = 0.2\n",
        "        else: \n",
        "            dropout_val = 0\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "        self.lstm2 = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "\n",
        "    def call(self, x, encoder_states):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        e_outputs, h1, c1 = self.lstm1(x, initial_state=encoder_states[0])\n",
        "\n",
        "        whole_sequence_output, h2, c2 = self.lstm2(e_outputs, initial_state=encoder_states[1])\n",
        "\n",
        "        encoder_states = [[h1, c1], [h2, c2]]\n",
        "\n",
        "        return whole_sequence_output, encoder_states\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return [[tf.zeros((self.batch_sz, self.enc_units)) for i in range(2)] for j in range(2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "outputId": "1dc3268a-9d74-428f-c55c-080d5a1d53df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Démo Encoder \n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, DROPOUT)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden_states = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder state shape: h2 : (batch size, units) {}'.format(sample_hidden_states[1][0].shape))\n",
        "print ('Encoder state shape: c2 : (batch size, units) {}'.format(sample_hidden_states[1][1].shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 12, 1000)\n",
            "Encoder state shape: h2 : (batch size, units) (128, 1000)\n",
            "Encoder state shape: c2 : (batch size, units) (128, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZaYwwKWqCSyy"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.W_p = tf.keras.layers.Dense(units)\n",
        "        self.v_p = tf.keras.layers.Dense(1)\n",
        "        self.W_a = tf.keras.layers.Dense(units)\n",
        "\n",
        "\n",
        "    def non_zero_softmax_on_row(self, row_tensor): \n",
        "        # Inputs : \n",
        "        #   row_tensor shape : (max_length_sentence, 1)\n",
        "\n",
        "        # Création d'une partition basée sur la condition non_zeros :\n",
        "        non_zeros_bool = ~tf.equal(row_tensor, 0.)\n",
        "        non_zeros_int = tf.cast(non_zeros_bool, tf.int32)\n",
        "        partitioned_tensor = tf.dynamic_partition(row_tensor, non_zeros_int, 2)\n",
        "\n",
        "        # Application du softmax seulement sur les valeurs non égales à 0.\n",
        "        partitioned_tensor[1] = tf.nn.softmax(partitioned_tensor[1])\n",
        "\n",
        "        # Trouver les indices à remplacer les résultats du softmax\n",
        "        condition_indices = tf.dynamic_partition(tf.range(tf.size(row_tensor)), tf.reshape(non_zeros_int, [-1]), 2)\n",
        "\n",
        "        # Effectuer le remplacement\n",
        "        softmax_row_tensor = tf.dynamic_stitch(condition_indices, partitioned_tensor)\n",
        "        softmax_row_tensor = tf.math.abs(softmax_row_tensor) # remove -0.0 values\n",
        "        softmax_row_tensor = tf.reshape(softmax_row_tensor, tf.shape(row_tensor))\n",
        "\n",
        "        return softmax_row_tensor\n",
        "\n",
        "    def non_zero_softmax_on_matrix(self, matrix_tensor):\n",
        "        return tf.map_fn(self.non_zero_softmax_on_row, matrix_tensor)\n",
        "\n",
        "    def call(self, dec_hidden_last_layer, enc_output):\n",
        "        # Inputs : \n",
        "        #   dec_hidden = ht du dernier layer  -> (batch_size, hidden_size)\n",
        "        #   enc_output = all hs               -> (batch_size, max_len, hidden_size) \n",
        "        #   S est la longueur des phrases (longueur de la plus longue phrase)\n",
        "        #   D est la taille de la fenetre (window)\n",
        "\n",
        "        batch_size = enc_output.shape[0]\n",
        "        S = enc_output.shape[1] \n",
        "        D = 10.0\n",
        "        sigma = D / 2.0\n",
        "\n",
        "        dec_hidden_last_layer_time_axis = tf.expand_dims(dec_hidden_last_layer, axis=1) # (batch_size, 1, hidden_size)   \n",
        "\n",
        "        # Trouvons pt. pt = S * sig(vp * tanh(Wp ht))\n",
        "        pt = self.W_p(dec_hidden_last_layer_time_axis)  \n",
        "        pt = tf.keras.activations.tanh(pt)                      # (batch_size, 1, hidden_size)\n",
        "        pt = self.v_p(pt)                                       # (batch_size, 1, 1)\n",
        "        pt = tf.keras.activations.sigmoid(pt)                   # (batch_size, 1, 1)\n",
        "        pt = pt * (S-1)                                         # (batch_size, 1, 1)   \n",
        "\n",
        "\n",
        "        pt_copy = tf.identity(pt)                               # (batch_size, 1, 1)\n",
        "        multiplies = tf.constant([1, S, 1], tf.int32)  \n",
        "        pt_matrix = tf.tile(pt_copy, multiplies)                # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir une matrice de contenant les indices de chaque colonne dans chaque ligne\n",
        "        row_indexes = [np.arange(S)]\n",
        "        matrix_row_indexes = np.repeat(row_indexes, batch_size, axis=0)                 # (batch_size, max_sent_length, 1)\n",
        "        matrix_row_indexes = np.expand_dims(matrix_row_indexes, axis=2)\n",
        "        matrix_row_indexes = tf.convert_to_tensor(matrix_row_indexes, dtype=tf.float32)\n",
        "\n",
        "        # Trouvons la matrice window_one_hot_matrix avec un 1 aux indices des mots qui doivent\n",
        "        # être considérés pour le calcul de l'attention\n",
        "        window_pt_diff = tf.math.subtract(matrix_row_indexes, pt)\n",
        "        window_pt_abs = tf.math.abs(window_pt_diff)\n",
        "        window_one_hot_matrix = tf.where(tf.less(window_pt_abs, D), 1.0, 0.0)           # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir poids de la gaussienne\n",
        "        s_minus_pt = tf.math.subtract(matrix_row_indexes, pt_matrix)\n",
        "        s_minus_pt_pow_2 = tf.map_fn(lambda x: x*x, s_minus_pt)\n",
        "        num_divided_by_denum = tf.math.divide(s_minus_pt_pow_2, 2*(sigma**2))\n",
        "        gaussian = tf.math.exp(-num_divided_by_denum)                                   # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir poids de la gaussienne seulement pour les mots faisant partie \n",
        "        # du calcul de l'attention\n",
        "        gaussian_window = tf.math.multiply(gaussian, window_one_hot_matrix)             # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Calcul du score\n",
        "        score = self.W_a(enc_output)    # score (Wa@hs) score.shape = (batch_size, max_sent_length, hidden_size)\n",
        "        score = tf.keras.layers.Dot(axes=[2, 2])([score, dec_hidden_last_layer_time_axis])  # score ht.T Wa@hs score.shape = (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Obtenir seulement les score faisant partie des mots à considérer\n",
        "        # pour le calcul de l'attention\n",
        "        score_one_hot = tf.math.multiply(window_one_hot_matrix, score)\n",
        "\n",
        "        # Appliquer softmax seulement sur les éléments de chaque ligne non égal à 0.\n",
        "        # Ces éléments représentent le score de chaque mot qui doit être considéré \n",
        "        # pour le calcul de l'attention\n",
        "        align_ht_hs = self.non_zero_softmax_on_matrix(score_one_hot)\n",
        "\n",
        "        # Obtenir les poids de l'attention en multipliant élément par élément \n",
        "        # les poids de la gaussienne au éléments de align_ht_hs\n",
        "        attention_weights = tf.math.multiply(gaussian_window, align_ht_hs) # (batch_size, max_sent_length, 1)\n",
        "\n",
        "        # Trouver ct = (batch_size, hidden_size) \n",
        "        # Détails : hs = (batch_size, max_sent_length, hidden_size) * at = (batch_size, max_sent_length, 1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1) # (BATCH_SIZE, 1000)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "outputId": "2eda5a3e-9af3-42a4-a74a-6b3e8ee405b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Démo Attention Layer \n",
        "\n",
        "attention_layer = LuongAttention(units)\n",
        "\n",
        "def get_dec_hidden_last_layer_time_axis():\n",
        "    return tf.random.uniform((BATCH_SIZE, units), minval=0, maxval=1, dtype=tf.float32, seed=64)\n",
        "\n",
        "dec_hidden_last_layer_time_axis = get_dec_hidden_last_layer_time_axis()\n",
        "attention_result, attention_weights = attention_layer(dec_hidden_last_layer_time_axis, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (128, 1000)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 12, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPKF5EWnCYV4"
      },
      "source": [
        "### Décodeur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.dropout = dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.W_c = tf.keras.layers.Dense(self.dec_units)\n",
        "\n",
        "        if dropout:\n",
        "            dropout_val = 0.2\n",
        "        else: \n",
        "            dropout_val = 0\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "        self.lstm2 = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          dropout = dropout_val)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = LuongAttention(self.dec_units)\n",
        "\n",
        "\n",
        "    def call(self, dec_input, dec_hidden, enc_output):\n",
        "        # Inputs:\n",
        "        #   dec_input shape == (batch_size, max_length, dec_hidden_size)\n",
        "        #   dec_hidden shape == (batch_size, dec_hidden_size)\n",
        "        #   enc_output shape == (batch_size, max_length, dec_hidden_size)\n",
        "\n",
        "        # x après le layer de embeddings == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(dec_input)\n",
        "\n",
        "        d_outputs, dh1, dc1 =  self.lstm1(x, initial_state=dec_hidden[0])\n",
        "        d_outputs_final, dh2, dc2 = self.lstm2(d_outputs, initial_state=dec_hidden[1]) \n",
        "\n",
        "        decoder_states = [[dh1, dc1], [dh2, dc2]]\n",
        "\n",
        "        if ATTENTION:\n",
        "            # Évaluer ht_hat = tanh(Wc[concat(ct,ht)])\n",
        "\n",
        "            # output shape == (batch_size * 1, hidden_size)\n",
        "            output = tf.reshape(d_outputs_final, (-1, d_outputs_final.shape[2]))\n",
        "\n",
        "            context_vector, attention_weights = self.attention(output, enc_output) \n",
        "            ct_ht = tf.concat([context_vector, output], axis=-1) # ct_ht shape == (batch_size, embedding_dim + dec_hidden_size)\n",
        "            Wc_ct_ht = self.W_c(ct_ht)\n",
        "            ht_hat =  tf.keras.activations.tanh(Wc_ct_ht)   # ht_hat shape == (batch_size, embedding_dim)\n",
        "\n",
        "            # output shape == (batch_size, embedding_dim)\n",
        "            output = ht_hat\n",
        "        else: \n",
        "            attention_weights = None\n",
        "            # output shape == (batch_size * 1, embedding_dim)\n",
        "            output = tf.reshape(d_outputs_final, (-1, d_outputs_final.shape[2]))\n",
        "\n",
        "\n",
        "        Ws_ht = self.fc(output) # Ws_ht shape == (batch_size, vocab)\n",
        "\n",
        "        return Ws_ht, decoder_states, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "outputId": "370ae3bc-1e3b-4379-ab04-bedc8df3736b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Démo Decoder \n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, DROPOUT)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden_states, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 11190)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Fonction de perte et optimiseur "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Entraînement\n",
        "\n",
        "1. Passez *l'entrée* à travers *l'encodeur* qui retourne *la sortie de l'encodeur* et *l'état caché de l'encodeur*.\n",
        "2. La sortie de l'encoder, l'état caché de l'encodeur et l'entrée du décodeur (qui est le token *start*) sont transmis au décodeur.\n",
        "3. 3. Le décodeur renvoie les prédictions et l'état caché du décodeur.\n",
        "4. L'état caché du décodeur est ensuite passée une fois de plus dans le modèle et les prédicitions sont utilisées pour évaluer la loss.\n",
        "5. Le principe de *teacher forcing* est utilisé pour déterminer le prochain entré du décodeur.\n",
        "6. La dernière étape consiste à évaluer le gradient et l'appliquer à l'optimisateur pour ensuite procéder à la backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "# La boucle d'entraînment provient du tutoriel Neural machine translation with attention \n",
        "# et a été adapté pour notre architecture.\n",
        "# Lien : https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, target, enc_states):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_states = encoder(inp, enc_states)\n",
        "\n",
        "        dec_hidden = enc_states\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, target.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "outputId": "b07c996c-de77-4b84-da7a-1881007788fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "col_names =  ['Epoch', 'Loss']\n",
        "loss_df  = pd.DataFrame(columns = col_names)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_states = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_states)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0: \n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        " \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "           \n",
        "    loss_df.loc[len(loss_df)] = [epoch + 1, K.eval(total_loss / steps_per_epoch)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 1.5953\n",
            "Time taken for 1 epoch 143.84101009368896 sec\n",
            "\n",
            "Epoch 2 Loss 1.0333\n",
            "Time taken for 1 epoch 112.0971736907959 sec\n",
            "\n",
            "Epoch 3 Loss 0.8147\n",
            "Time taken for 1 epoch 109.88107943534851 sec\n",
            "\n",
            "Epoch 4 Loss 0.6581\n",
            "Time taken for 1 epoch 112.11165642738342 sec\n",
            "\n",
            "Epoch 5 Loss 0.5328\n",
            "Time taken for 1 epoch 109.66650485992432 sec\n",
            "\n",
            "Epoch 6 Loss 0.4286\n",
            "Time taken for 1 epoch 112.80205583572388 sec\n",
            "\n",
            "Epoch 7 Loss 0.3423\n",
            "Time taken for 1 epoch 110.29715061187744 sec\n",
            "\n",
            "Epoch 8 Loss 0.2726\n",
            "Time taken for 1 epoch 112.55710554122925 sec\n",
            "\n",
            "Epoch 9 Loss 0.2181\n",
            "Time taken for 1 epoch 109.65402913093567 sec\n",
            "\n",
            "Epoch 10 Loss 0.1767\n",
            "Time taken for 1 epoch 110.79903197288513 sec\n",
            "\n",
            "Epoch 11 Loss 0.1461\n",
            "Time taken for 1 epoch 107.93765258789062 sec\n",
            "\n",
            "Epoch 12 Loss 0.1232\n",
            "Time taken for 1 epoch 110.89509606361389 sec\n",
            "\n",
            "Epoch 13 Loss 0.1076\n",
            "Time taken for 1 epoch 109.14932990074158 sec\n",
            "\n",
            "Epoch 14 Loss 0.0962\n",
            "Time taken for 1 epoch 112.06437683105469 sec\n",
            "\n",
            "Epoch 15 Loss 0.0886\n",
            "Time taken for 1 epoch 109.86105036735535 sec\n",
            "\n",
            "Epoch 16 Loss 0.0833\n",
            "Time taken for 1 epoch 112.2155408859253 sec\n",
            "\n",
            "Epoch 17 Loss 0.0801\n",
            "Time taken for 1 epoch 109.88406896591187 sec\n",
            "\n",
            "Epoch 18 Loss 0.0776\n",
            "Time taken for 1 epoch 111.13199734687805 sec\n",
            "\n",
            "Epoch 19 Loss 0.0754\n",
            "Time taken for 1 epoch 108.23537731170654 sec\n",
            "\n",
            "Epoch 20 Loss 0.0733\n",
            "Time taken for 1 epoch 111.12959337234497 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AI_IaVxCHF5z",
        "colab": {}
      },
      "source": [
        "if ATTENTION:\n",
        "    csv_name = \"tableau_loss_with_attention.csv\"\n",
        "    loss_df.to_csv(csv_name)\n",
        "    !cp tableau_loss_with_attention.csv \"drive/My Drive/\"\n",
        "else:  \n",
        "    csv_name = \"tableau_loss_without_attention.csv\"    \n",
        "    loss_df.to_csv(csv_name)\n",
        "    !cp tableau_loss_without_attention.csv \"drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Traduction\n",
        "\n",
        "* La fonction d'évaluation est similaire à la boucle d'entraînement, sauf que nous n'utilisons pas le *teacher forcing*. L'entrée du décodeur à chaque pas de temps t correspond à ses prédictions précédentes ainsi qu'à l'état caché et à la sortie de l'encodeur.\n",
        "* La prédiction pour la traduction arrête lorsque le *end token* est atteint.\n",
        "* Les *attention weights* sont enregistrés pour chaque instant t.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "# Les fonctions utilisées pour la traduction proviennent du tutoriel Neural machine translation with attention \n",
        "# et ont été adaptées pour notre architecture et nos traitements.\n",
        "# Lien : https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence, REVERSE)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [[tf.zeros((1, units)) for i in range(2)] for j in range(2)]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        if attention_weights is None:\n",
        "            attention_plot = None\n",
        "        else:\n",
        "            attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "            attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence, print_attention=True):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    if attention_plot is not None and print_attention:\n",
        "        attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "        plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restoration du dernier checkpoint pour tester"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "outputId": "3b9242d9-ef1e-4839-9875-217f789236ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fef33d95160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZM0Vj_jRGF7",
        "outputId": "23313942-71a4-4232-f27e-61b4804ec870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I recovered')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je m en suis remise . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0BDjy2ew0Kkm",
        "outputId": "2a196529-420a-4646-9fed-399c4e9f3207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'You re the oldest.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tu es le plus vieux . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izleX7rf0KiT",
        "outputId": "1f685662-f521-4e20-f282-d09165b60151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'He walks to school .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'il marche vers l ecole . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2wJe8Oh0Kg1",
        "outputId": "c2866b91-5735-4f64-a95e-23a3749b0705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'Make a list .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'faites une liste ! <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rkGG1vXL0Kdp",
        "outputId": "1687c7e8-dbee-466f-a267-e38994914c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'Is it your car .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'c est votre voiture . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YBsQmkhT0KbW",
        "outputId": "dd2730ce-e44d-4c12-a171-61377f7c9747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'He lied to me .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'il m a menti . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOu-FwxL0KZH",
        "outputId": "78be2fec-5e73-466b-c525-0a96fa0eb725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I didn t laugh .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je n ai pas ri . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p22f-uOi0KWd",
        "outputId": "535a4e3c-4207-490a-9b83-85a90e9b691f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I work with him .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je travaille avec lui . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DTJcyc-c0KTz",
        "outputId": "e947f22f-c2ac-4560-bf14-a9be0ab00aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I just moved here .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je viens de demenager ici . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2uClBJEu0KLZ",
        "outputId": "dc7ddd45-9dfd-49cc-8d95-166f39434b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I felt left out .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je me suis senti delaisse . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zI4ENwFz1vpH",
        "outputId": "d974bcc0-166e-423b-f402-980c5b2139d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'What did you make .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'que vous avez fait . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwwkmI-NZh7j",
        "colab_type": "code",
        "outputId": "95ecf6b4-0951-4871-92a6-cf054edba9a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I want to be a student .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je veux etre fermier . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2W2pbIOZs4h",
        "colab_type": "code",
        "outputId": "a04b2b49-75b6-450c-86ce-222f3c334bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I want to be a good student .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je veux etre une bonne etudiante . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE1s6thWZt5z",
        "colab_type": "code",
        "outputId": "5bb0fc10-a0f9-49dc-b875-deb587369bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "translate(u'I want to be a bad student .')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je veux etre une mauvaise personne . <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j2D32bYB4mWZ"
      },
      "source": [
        "## BLEU score sur test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JADVb-8L4qX4",
        "colab": {}
      },
      "source": [
        "# input_tensor_val: phrase en entrée\n",
        "# target_tensor_val : phrase attendue\n",
        "\n",
        "smoother = SmoothingFunction()\n",
        "\n",
        "BLEU_scores = {}\n",
        "\n",
        "col_names =  ['Length', 'BLEUscore']\n",
        "BLEU_df  = pd.DataFrame(columns = col_names)\n",
        "\n",
        "\n",
        "col_names =  ['id', 'src', 'result', 'reference_str', 'sentence_length', 'BLEUscore']\n",
        "sentence_sample_df  = pd.DataFrame(columns=col_names)\n",
        "\n",
        "\n",
        "#for i in range(len(input_tensor_val)):\n",
        "nb_examples = len(input_tensor_val)\n",
        "\n",
        "\n",
        "for i in range(nb_examples):\n",
        "    # On fait la traduction\n",
        "    converted_sentence = tensor_to_sentence(inp_lang, input_tensor_val[i])\n",
        "    sentence_length = str(len(converted_sentence))\n",
        "\n",
        "    src = \" \".join(converted_sentence)\n",
        "    result = translate(src, print_attention=False).strip()\n",
        "    hypothesis = result.split(\" \")[:-1]\n",
        "    reference = tensor_to_sentence(targ_lang, target_tensor_val[i])\n",
        "    reference_str = \" \".join(reference)\n",
        "\n",
        "\n",
        "    # Il peut y avoir plusieurs références, mais pas dans notre cas\n",
        "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, smoothing_function=smoother.method1)\n",
        "\n",
        "    # Enregister certaines traductions pour une analyse manuelle\n",
        "    percentage = 0.01\n",
        "    if i % (int(percentage * nb_examples)) == 0:\n",
        "        idx = i // (int(percentage * nb_examples))\n",
        "        sentence_sample_df.loc[idx] = [idx, src, result, reference_str, sentence_length, BLEUscore] \n",
        "\n",
        "    if str(sentence_length) in BLEU_scores:\n",
        "        occurences = BLEU_scores[sentence_length][0] + 1\n",
        "        total = BLEU_scores[sentence_length][1] + BLEUscore\n",
        "        BLEU_scores[sentence_length] = (occurences, total)\n",
        "    else:\n",
        "        BLEU_scores[sentence_length] = (1, BLEUscore)\n",
        "\n",
        "lengths = []\n",
        "BLEU_averages = []\n",
        "\n",
        "for sentence_length, (occurences, total_BLEU_score) in BLEU_scores.items():\n",
        "    length = int(sentence_length)\n",
        "    average_BLEU_score = total_BLEU_score / occurences\n",
        "    lengths.append(length)\n",
        "    BLEU_averages.append(average_BLEU_score)\n",
        "    BLEU_df.loc[len(BLEU_df)] = [length, average_BLEU_score] \n",
        "\n",
        "lists = sorted(zip(*[lengths, BLEU_averages]))\n",
        "lengths, BLEU_averages = list(zip(*lists))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qTu4s5-THR-I",
        "colab": {}
      },
      "source": [
        "if ATTENTION:\n",
        "    csv_name = \"tableau_BLEU_with_attention.csv\"\n",
        "    #ouput_dataframe(BLEU_df, csv_name)\n",
        "    BLEU_df.to_csv(csv_name)\n",
        "    !cp tableau_BLEU_with_attention.csv \"drive/My Drive/\"\n",
        "\n",
        "    csv_name = \"tableau_traductions_with_attention.csv\"\n",
        "    #ouput_dataframe(sentence_sample_df, csv_name)\n",
        "    sentence_sample_df.to_csv(csv_name)\n",
        "    !cp tableau_traductions_with_attention.csv \"drive/My Drive/\"\n",
        "\n",
        "else:\n",
        "    csv_name = \"tableau_BLEU_without_attention.csv\"\n",
        "    #ouput_dataframe(BLEU_df, csv_name)\n",
        "    BLEU_df.to_csv(csv_name)\n",
        "    !cp tableau_BLEU_without_attention.csv \"drive/My Drive/\"\n",
        "\n",
        "    csv_name = \"tableau_traductions_without_attention.csv\"\n",
        "    #ouput_dataframe(sentence_sample_df, csv_name)\n",
        "    sentence_sample_df.to_csv(csv_name)\n",
        "    !cp tableau_traductions_without_attention.csv \"drive/My Drive/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lQqxFrXD3Glx",
        "outputId": "c158cb2e-a25b-465c-9132-3fe4b27a2665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(lengths, BLEU_averages)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fef32c27240>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jUWMn-Nj0dFJ",
        "outputId": "89c2eab3-24ec-4652-a29e-b6fc4499b899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "global_BLEU_score = 0\n",
        "global_occurences = 0\n",
        "\n",
        "for sentence_length, (occurences, total_BLEU_score) in BLEU_scores.items():\n",
        "    global_BLEU_score += total_BLEU_score\n",
        "    global_occurences += occurences\n",
        "\n",
        "print(\"Average BLEU score:\", global_BLEU_score / global_occurences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average BLEU score: 0.30165615835180754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuIaNtCoy_d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}